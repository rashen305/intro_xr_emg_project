# XR EMG Project
This project seeks to perform real-time hand gesture identification, including pinching and force estimation.
We use an 8-channel sEMG for collecting signal data, machine learning for classification/inferencing, and Unity to demonstrate controls.

TCP sockets are used to communicate between the three segments of the pipeline.

`C++ (sEMG) â†’ Python (neural inferencing) â†’ C# (Unity scene)`


## ðŸ“‚ Repository Structure
```
intro_xr_emg_project/   
â”œâ”€â”€ README.md           # you are here
â”œâ”€â”€ Unity/              # XR Scene for demonstrating gestures
â”œâ”€â”€ data_transmission/  # Currently a scratch folder
â”œâ”€â”€ environment.yaml    # Quickly install conda environment with all required python dependencies
â”œâ”€â”€ ml/                 # All logic for training/evaluating/inferencing a neural network on sEMG data
â”œâ”€â”€ myo/                # sEMG data collection through the Myo SDK
â””â”€â”€ socket_folder/      # Currently a scratch folder
```

---

## ðŸ§­ Overview
| Directory/File | Purpose |
| :--- | :--- |
| **`ml/`** | Contains the core PyTorch model, preprocessing scripts, and training/evaluation notebooks for EMG classification. |
| **`myo/`** | Stores raw EMG sample data and potentially Myo-specific SDK integration files. |
| **`Unity/`** | Contains the Unity project files, likely handling the 3D or XR visualization components. |
| **`environment.yaml`** | Conda or pip environment definition file, listing all required Python dependencies. |