{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  EMG Spectrogram CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import the object-oriented model and preprocessing utility\n",
    "from cnn_model import CNNmodel\n",
    "from emg_preprocessing import preprocess, EMGDataset, FS, WINDOW_SIZE, STRIDE, NPERSEG, NOVERLAP, N_CHANNELS\n",
    "\n",
    "print(\"Libraries and modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfe4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training knobs ---\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3   # 0.001\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Labels and Class Names ---\n",
    "LABELS = {\"rest\": 0, \"pinch\": 1}\n",
    "CLASS_NAMES = [\"rest\", \"pinch\"]\n",
    "\n",
    "# Placeholder for actual data paths\n",
    "# todo: centralize data location, e.g. ../data/\n",
    "DATA_FILES = [\n",
    "    (\"../myo/samples/raymond_arm_90_deg_200hz.csv\", LABELS[\"rest\"]),\n",
    "    (\"../myo/samples/raymond_arm_90_deg_pinch_200hz.csv\", LABELS[\"pinch\"]),\n",
    "    (\"../myo/samples/raymond_arm_down_200hz.csv\", LABELS[\"rest\"]),\n",
    "    (\"../myo/samples/raymond_arm_down_pinch_200hz.csv\", LABELS[\"pinch\"])\n",
    "]\n",
    "\n",
    "# Device Setup (Prioritizing MPS for Apple Silicon)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Functions and Dataset Class\n",
    "*(This logic is now defined in `emg_preprocessing.py`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing functions imported from emg_preprocessing.py.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load, Normalize, and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "\n",
    "print(\"Preprocessing all data files...\")\n",
    "for path, label in DATA_FILES:\n",
    "    print(f\"Processing file: {path} with label: {label}\")\n",
    "    X_specs = preprocess(path)\n",
    "    if X_specs.size > 0:\n",
    "        X_all.append(X_specs)\n",
    "        Y_all.append(np.full(X_specs.shape[0], label, dtype=np.int64))\n",
    "\n",
    "X_all = np.concatenate(X_all)\n",
    "Y_all = np.concatenate(Y_all)\n",
    "\n",
    "print(f\"Total windows collected: {len(X_all)}\")\n",
    "\n",
    "# 1. Calculate Global Normalization Parameters\n",
    "mean = X_all.mean(axis=(0, 2, 3), keepdims=True)\n",
    "std = X_all.std(axis=(0, 2, 3), keepdims=True)\n",
    "\n",
    "print(f\"Global Mean shape: {mean.shape}\")\n",
    "print(f\"Global Std shape: {std.shape}\")\n",
    "\n",
    "# Save normalization params for inference\n",
    "np.save(\"normalization_params.npy\", {'mean': mean, 'std': std})\n",
    "print(\"Normalization parameters saved to normalization_params.npy\")\n",
    "\n",
    "# 2. Apply Normalization\n",
    "X_all = (X_all - mean) / std\n",
    "\n",
    "# 3. Split Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_all, Y_all, test_size=0.2, random_state=42, stratify=Y_all\n",
    ")\n",
    "\n",
    "print(f\"Training windows: {len(X_train)}\")\n",
    "print(f\"Testing windows: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce089332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with built-in optimizer and loss function.\n",
      "CNNmodel(\n",
      "  (conv1): Conv2d(8, 32, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model (The CNNmodel class now handles its own criterion and optimizer)\n",
    "model = CNNmodel(\n",
    "    in_channels=N_CHANNELS, \n",
    "    num_classes=len(LABELS), \n",
    "    learning_rate=LR, \n",
    "    device=DEVICE\n",
    ") \n",
    "print(\"Model initialized with built-in optimizer and loss function.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training set: 123\n",
      "Number of batches in test set: 31\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets\n",
    "train_dataset = EMGDataset(X_train, Y_train)\n",
    "test_dataset = EMGDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test set: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9922a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "best_preds = None\n",
    "best_labels = None\n",
    "weights_path = \"train_single_subject_myo_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training for {EPOCHS} epochs on {DEVICE}...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train Epoch\n",
    "    train_loss, train_acc = model.train_epoch(train_loader)\n",
    "    \n",
    "    # Test/Validation Epoch\n",
    "    test_acc, preds_all, labels_all = model.test_epoch(test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Save best model checkpoint\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_preds = preds_all \n",
    "        best_labels = labels_all \n",
    "        torch.save(model.state_dict(), weights_path)\n",
    "        print(f\"Model saved to {weights_path} (Best Acc: {best_acc:.4f})\")\n",
    "print(\"\\n\\nðŸŽ‰ Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_preds is not None and best_labels is not None:\n",
    "    print(\"\\nBest Test Accuracy:\", best_acc)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    # Note: Labels [0, 1] correspond to CLASS_NAMES [\"rest\", \"pinch\"]\n",
    "    cm = confusion_matrix(best_labels, best_preds, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    plt.title(\"EMG CNN Classification â€” Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not generate confusion matrix. Check if training ran successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
